<!DOCTYPE html>
<html>
    <head>
        <meta charset='utf-8'>
        <meta http-equiv='X-UA-Compatible' content='IE=edge'>
        <meta name='viewport' content='width=device-width, initial-scale=1'>
        <meta name='description' content='ECE521 course website'>
        <title>ECE521 Inference Algorithms and Machine Learning (Winter 2017)</title>
        <link rel="shortcut icon" type="image/png" href='./assets/favicons.png'>
        <link href='./assets/app.css' rel='stylesheet' type='text/css'>
        <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]> <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script> <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script> <![endif]-->
    </head>
    <body>
        <nav role='navigation'>
            <div class='container'>
                <div class='navbar-header'>
                    <button type='button' data-toggle='collapse' data-target='#menu'>
                        <span class='sr-only'>Toggle navigation</span>
                        <span class='icon-bar'></span>
                        <span class='icon-bar'></span>
                        <span class='icon-bar'></span>
                    </button>
                    <a class='navbar-brand' href='#'>ECE521</a>
                </div>
                <div id='menu'>
                </div>
            </div>
        </nav>
        <header>
            <div class='headline'>
                <div class='container'>
                    <h1>University of Toronto
</h2><h2>ECE521 &#58;Inference Algorithms and Machine Learning</h2>
</div></div></header>
<div class="container">
    <main>
        <hr id='/contents/01-info'>
        <section>
            <h2>
                <span>Course &#58;</span>
            </h2>
            <div class='lead'>
                <p>
                    <strong>Instructors:</strong>
                    Jimmy Ba and Mark Ebden
                </p>
                <p>
                    <strong>Section1:</strong>
                    M 10 – 11 (BA1130) TH 9 – 11 (BA1130)
                </p>
                <p>
                    <strong>Section2:</strong>
                    M 11 – 12 (RS211)   TH 12 – 14 (LM161)
                </p>
                <p>
                    <strong>Office hours:</strong>
                    Jimmy: M 11-12 in BA4161   Mark: TH 15-16 in SS6026C
                </p>
                <p>
                    <strong>Tutorials:</strong>
                    F 10 – 12 (BA2155)   M 12 – 14 (BA2155)   F 16 – 18 (BA2155)   F 09 – 11 (<strong>GB248</strong>
                    ) (merged TUT0104 and TUT0105)
                </p>
                <p></p>
                <p>Instructors won’t stick strictly to teaching a given section. For example, on Thursday 19 January Mark Ebden will teach both sections (1 and 2), and in future sometimes Jimmy Ba will teach both. This will occur regularly.</p>
                <p>
                    <a href="">Course Piazza</a>
                    and <a href="./static/syllabus.pdf">Course Syllabus</a>
                </p>
                <p>Note that the time of the midterm has changed from what the syllabus initially said. It is now to begin in the evening but still lasting about 90 minutes.</p>
            </div>
        </section>
        <hr id='/contents/02-annoucements'>
        <section>
            <h2>
                <span>Announcements &#58;</span>
            </h2>
            <div class='lead'>
                <ul>
                    <li>
                        <p>
                            <strong>Apr 18, 6:45pm</strong>
                            : We will hold the following office hours to help preparing for the final exam. The final exam is scheduled for Thursday April 20th 9:30am- 12:00 noon
                        </p>
                    </li>
                    <li>
                        <strong>Apr 16, 4:29pm</strong>
                        : We will hold the following office hours to help preparing for the final exam. The final exam is scheduled for Thursday April 20th 9:30am- 12:00 noon 
                        <ul>
                            <li>Monday April 17th, BA4161</li>
                            <li>12:00- 1:00 pm: Kaustav, Tony</li>
                            <li>3:00-4:00 pm: Jimmy</li>
                            <li>Tuesday April 18th, BA4161</li>
                            <li>12:00- 1:00 pm: Renjie, Eleni</li>
                            <li>Wednesday April 19th, BA4161</li>
                            <li>12:00- 1:00 pm: Shenlong, Renjie</li>
                        </ul>
                    </li>
                    <li>
                        <p>
                            <strong>Mar 27, 1:34pm</strong>
                            : Edit: The assignment 4 handout is posted on the course website. The due date is April 9th midnight, 2017. You have two weeks to complete this assignment. There are a few practice problems included in the A4 handout that will not be graded. You do not need to include the solutions for the practice questions in your final report. These practice problems may be helpful for preparing the final exam on the 20th of April.
                        </p>
                    </li>
                    <li>
                        <p>
                            <strong>Mar 8, 12:45pm</strong>
                            : The assignment 3 pdf handout and the related dataset is out on the course website. The due date is Mar 24th midnight, 2017. You have more than two weeks to complete this assignment.
                        </p>
                    </li>
                    <li>
                        <p>
                            <strong>Feb 13, 3:27pm</strong>
                            : Skule has implemented a new on-going annoynemous feedback system <a href="http://speakup.skule.ca/">SpeakUp!</a>
                            . Instead of filling out the monthly early course evaluation in class, you can also submit your feedbacks on a rolling basis through the SpeakUp! webpage and your class rep will contact the course instructors to discuss the issues.
                        </p>
                    </li>
                    <li>
                        <p>
                            <strong>Feb 10, 1:18am</strong>
                            : The midterm questions from 2016 is posted. And you can find the midterm cheatsheet template on this page that you may enter information on both sides of the aid sheet, without restriction. The cheatsheet should be printed on 8.5″ x 11″ paper.
                        </p>
                    </li>
                    <li>
                        <p>
                            <strong>Feb 9, 11:37pm</strong>
                            : The assignment 2 pdf handout and the related dataset is out on the course website. The due date is Feb 27th midnight, 2017. You have more than two weeks to complete this assignment.
                        </p>
                    </li>
                    <li>
                        <p>
                            <strong>Feb 6, 3:00pm</strong>
                            : Typo correction for the bonus question 1.4.1 of the assignment 1, the \lambda should be set to 100 instead when you report the simulation results. You should see a smooth prediction from your Gaussian process regression model.
                        </p>
                    </li>
                    <li>
                        <p>
                            <strong>Feb 6, 9:27am</strong>
                            : We are trying to recruit a volunteer note-taker for the ECE521H1 section 0101 for students registered with Accessibility Service. Email <a href="mailto:as.notetaking@utoronto.ca">as.notetaking@utoronto.ca</a>
                            if you have questions or require any assistance. Thanks.
                        </p>
                    </li>
                    <li>
                        <p>
                            <strong>Jan 25, 2:29pm</strong>
                            : I apologize for the multiple delays and the assignment 1 handout is finally out. We have spent quite sometime designing the exercises in the assignment and we hope you may find it rewarding to solve them. The pdf handout and the related dataset can be downloaded from the calendar section at this course website. The due date is Feb 7 midnight, 2017. You have two weeks to complete this assignment. There is a bonus part in the assignment that is roughly equal to 30% of assignment 1. The bonus marks can be used towards the final grade of this course.
                        </p>
                    </li>
                    <li>
                        <p>
                            <strong>Jan 16, 6:18pm</strong>
                            : The Friday morning 9-11 tutorial sections TUT0104 and TUT0105 are now merged and will be held in GB248 from now on.
                        </p>
                    </li>
                    <li>
                        <p>
                            <strong>Jan 13, 1:20am</strong>
                            : The first tutorials start today. There is a new tutorial section TUT0105 for the students who could not find tutorial space before. The TUT0105 tutorial time this week is Fri 9-11 am in BA1240. The tutorial room may change in the following weeks. Stay tuned. The first week lecture slides are also posted below.
                        </p>
                    </li>
                    <li>
                        <strong>Jan 09, 8:23am</strong>
                        : Welcome to ECE521! Please take a moment to enroll the course <a href="https://piazza.com/utoronto.ca/winter2017/ece521/home">Piazza</a>
                        . Piazza will be the main communication channel to contact the instructors for this course. The first tutorials start on Friday.
                    </li>
                </ul>
            </div>
        </section>
        <hr id='/contents/03-overview'>
        <section>
            <h2>
                <span>Course Overview &#58;</span>
            </h2>
            <div class='lead'>
                <p>The twenty-first century has seen a series of breakthroughs in statistical machine learning and inference algorithms that allow us to solve many of the most challenging scientific and engineering problems in artificial intelligence, self-driving vehicles, robotics and DNA sequence analysis. In the past few years, machine learning applications in search engines, wearable devices and social networks have broadly impacted our daily life. These algorithms adapt to the data at hand and are tolerant to noisy observations. The goal of this course is to provide principled mathematical tools to solve statistical inference problems you may encounter later. The first half of the course covers the fundamentals of statistical machine learning and supervised learning models. The second half of the course focuses on probabilistic inference and unsupervised learning. The examples of the course include object recognition; image search, document retrieval; sequence filtering and alignment; and data compression. This course reviews state-of-the-art algorithms and models for probabilistic inference and machine learning.</p>
            </div>
        </section>
        <hr id='/contents/04-team'>
        <section>
            <h2>
                <span>Teaching Assistants &#58;</span>
            </h2>
            <div class='lead'>
                <p>Min Bai</p>
                <p>Sinisa Colic</p>
                <p>Kaustav Kundu</p>
                <p>Renjie Liao</p>
                <p>Mengye Ren</p>
                <p>Eleni Triantafillou</p>
                <p>Shenlong Wang</p>
                <p>Yuhuai Wu</p>
                <p>Tianrui Xiao</p>
                <p>Haiyan Xu</p>
                <p>
                    <strong>TA Contact:</strong>
                    Please post your questions related to tutorials, assignments and exams on <a href="https://piazza.com/utoronto.ca/winter2017/ece521/home">Piazza</a>
                    . Do NOT send emails about the class to the personal emails of the TAs directly. We will not answer.
                </p>
            </div>
        </section>
        <hr id='/contents/05-calendar'>
        <section>
            <h2>
                <span>Calendar &#58;</span>
            </h2>
            <div class='lead'>
                <table>
                    <thead>
                        <tr>
                            <th></th>
                            <th>Optional reading</th>
                            <th>Topic</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>
                                <strong>Week 1</strong>
                            </td>
                            <td></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>Lecture: Monday, Jan 09</td>
                            <td></td>
                            <td>
                                Introduction <br/>
                                <a href="./static/Lec1-intro.pdf">pdf slides</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Lecture: Thursday, Jan 12</td>
                            <td></td>
                            <td>
                                Review of probability <br/>
                                Fundamentals of machine learning <br/>
                                <a href="./static/Lec2-fundamental.pdf">pdf slides</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Tutorial</td>
                            <td></td>
                            <td>
                                Review of linear algebra <br/>
                                Introduction to TensorFlow <br/>
                                <a href="./static/Tut1.pdf">pdf slides</a>
                                <br/>
                                <a href="./static/tutorial1.ipynb">Tutorial examples</a>
                                as an IPython Notebook. <br/>
                                Take a look at <a href="http://jupyter.readthedocs.io/en/latest/install.html">here</a>
                                on how to install Jupyter/IPython Notebook.
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <strong>Week 2</strong>
                            </td>
                            <td></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>Lecture: Monday, Jan 16</td>
                            <td>
                                The curse of dimensionality: <em>Bishop 2006, Chap. 1.4</em>
                                <br/>
                                K-NN: <em>Bishop 2006, Chap. 2.5.2</em>
                                <br/>
                                (free) K-NN and linear regression: <em>Hastie et al 2013, Chap. 2.3</em>
                                <br/>
                                (free) Convex function and Jensen inequality: <em>MacKay 2003, Chap. 2.7</em>
                                <br/>
                                (free) Gradient descent: <em>Goodfellow et al 2016, Chap. 4.3</em>
                            </td>
                            <td>
                                Example: K Nearest Neighbours <br/>
                                Optimization <br/>
                                <a href="./static/Lec3-kNNoptim.pdf">pdf slides</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Lecture: Thursday, Jan 19</td>
                            <td>
                                <a href="http://cilvr.cs.nyu.edu/diglib/lsml/bottou-sgd-tricks-2012.pdf">Stochastic gradient descent, Léon Bottou</a>
                                <br/>
                                <a href="https://www.youtube.com/watch?v=LdkkZglLZ0Q">
                                    The momentum method: <em>Coursera video: Neural Networks for Machine Learning Lecture 6.3</em>
                                </a>
                                <br/>
                                <a href="http://www.inference.phy.cam.ac.uk/itprnn/book.pdf">
                                    Maximum likelihood for a Gaussian: <em>MacKay 2003, Chap. 22.1</em>
                                </a>
                                <br/>
                                <a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf">
                                    Maximum likelihhod estimation of a classifier: <em>Hastie et al 2013, Chap. 2.6.3</em>
                                </a>
                                <br/>
                                <a href="http://www.deeplearningbook.org/contents/regularization.html">
                                    Regularization: <em>Goodfellow et al 2016, Chap. 7.1</em>
                                </a>
                                <br/>
                                <a href="http://www.deeplearningbook.org/contents/regularization.html">
                                    Regularization through data augmentation: <em>Goodfellow et al 2016, Chap. 7.4</em>
                                </a>
                            </td>
                            <td>
                                Maximum likelihood estimation (MLE)]<br/>
                                Optimization and regularization <br/>
                                <a href="./static/Lec4.pdf">pdf slides</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Tutorial</td>
                            <td></td>
                            <td>
                                Tricks to improve SGD <br/>
                                “Tuning/debugging” optimizer <br/>
                                Multivariate Gaussian <br/>
                                Underfitting vs. overfitting <br/>
                                <a href="./static/Tut2.pdf">pdf slides</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <strong>Week 3</strong>
                            </td>
                            <td></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>Lecture: Monday, Jan 23</td>
                            <td></td>
                            <td>
                                Probabilistic interpretation of linear regression <br/>
                                MLE vs. MAP <br/>
                                Optimal regressor <br/>[pdf slides] (see the joint slide deck from Jan 26)
                            </td>
                        </tr>
                        <tr>
                            <td>Assignment 1: Wednesday, Jan 25</td>
                            <td>
                                Due date: Feb 7 midnight, 2017 <br/>k-NN, Gaussian process (bonus), linear regression
                            </td>
                            <td>
                                <a href="./static/a1.pdf">Assignment handout</a>
                                <br/>
                                Download Tiny MNIST dataset <a href="./static/tinymnist.npz">here</a>
                                <br/>
                                <a href="./static/histAss1_ECE.png">Histogram of results</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Lecture: Thursday, Jan 26</td>
                            <td>
                                Regression and decision theory: <em>Bishop 2006, Chap. 1.5</em>
                                <br/>
                                Bias-variance trade-off: <em>Bishop 2006, Chap. 3.2</em>
                            </td>
                            <td>
                                Optimal regressor <br/>
                                Feature expansion <br/>
                                Decision theory <br/>
                                <a href="./static/Lec5and6.pdf">joint pdf slides</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Tutorial</td>
                            <td></td>
                            <td>
                                k-NN, Linear regression <br/>
                                Gaussian process regression <br/>
                                Training, validation and test set <br/>
                                <a href="./static/Tut3.pdf">pdf slides</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <strong>Week 4</strong>
                            </td>
                            <td></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>Lecture: Monday, Jan 30</td>
                            <td></td>
                            <td>
                                Recap decision theory <br/>
                                Logistic regression <br/>
                                Neural networks <br/>
                                <a href="./static/Lec7-logistic.pdf">pdf slides</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Lecture: Thursday, Feb 2</td>
                            <td>
                                Neural networks: <em>Bishop 2006, Chap. 5</em>
                                , <em>MacKay 2003, Chap. 39-40, 44</em>
                                , <em>Hastie et al 2013, Chap. 11</em>
                                <br/>
                            </td>
                            <td>
                                Neural networks <br/>
                                Backpropagation <br/>
                                <a href="./static/Lec8-nnP.pdf">pdf slides</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Tutorial</td>
                            <td></td>
                            <td>
                                Logistic regression <br/>
                                Backpropagation examples <br/>
                                <a href="./static/Tut4.pdf">pdf slides</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <strong>Week 5</strong>
                            </td>
                            <td></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>Lecture: Monday, Feb 6</td>
                            <td></td>
                            <td>
                                Multi-class classification <br/>
                                Learning feedforward neural networks <br/>
                                <a href="./static/Lec9-nn2.pdf">pdf slides</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Lecture: Thursday, Feb 9</td>
                            <td>
                                <a href="http://cs231n.github.io/convolutional-networks/">Convolutional neural networks: cs231n course slides</a>
                                <br/>
                                <a href="http://cs231n.github.io/transfer-learning/">Transfer learning and fine-tuning: cs231n course slides</a>
                            </td>
                            <td>
                                Bag-of-tricks for deep neural networks <br/>
                                Types of neural networks: convolutional neural networks, recurrent neural networks <br/>
                                <a href="./static/Lec10-nn3.pdf">pdf slides</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Assignment 2: Thursday, Feb 9</td>
                            <td>
                                Due date: Feb 27 midnight, 2017 <br/>logistic regression, neural networks
                            </td>
                            <td>
                                <a href="./static/a2.pdf">Assignment handout (updated Feb 18th)</a>
                                <br/>
                                Download notMNIST dataset <a href="./static/notMNIST.npz">here</a>
                                <br/>
                                <a href="./static/histAss2_ECE.png">Histogram of results</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Tutorial</td>
                            <td></td>
                            <td>
                                Sample midterm review <br/>Assignment 1 post-mortem
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <strong>Week 6</strong>
                            </td>
                            <td></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>Lecture: Mon, Feb 13</td>
                            <td>Bishop 9.1 and 12.1         </td>
                            <td>
                                <a href="./static/Lec11.pdf">k-means clustering, dimensionality reduction</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Study: Thu, Feb 16</td>
                            <td></td>
                            <td>Study independently in the classroom, with instructor on hand for questions. Unstructured.</td>
                        </tr>
                        <tr>
                            <td>Midterm: Thursday, Feb 16</td>
                            <td>Time: 6:20-7:50 pm.</td>
                            <td>
                                <a href="./static/ECE521_midterm_2016.pdf">Sample midterm from 2016</a>
                                <br/>
                                <a href="http://undergrad.engineering.utoronto.ca/wp-content/uploads/2016/06/Examination-Aid-Sheet-0215.pdf">Midterm cheatsheet template</a>
                                <br/>
                                <a href="./static/histMidtermECE.png">Histogram of results</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Tutorial</td>
                            <td></td>
                            <td>Midterm exam post-mortem</td>
                        </tr>
                        <tr>
                            <td>
                                <strong>Week 7</strong>
                            </td>
                            <td></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>Lecture: Mon, Feb 27</td>
                            <td>
                                Bishop 3.3<br/>Murphy 2012: parts of chap. 5 &amp;sec. 7.6
                            </td>
                            <td>
                                <a href="./static/Lec12.pdf">PCA continued, Bayesian methods</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Lecture: Thu, Mar 2</td>
                            <td>Bishop 1.2.6 (Bayesian prediction), 1.3 (model selection), 2.4.2 (conjugate prior)</td>
                            <td>
                                <a href="./static/Lec12.pdf">Bayesian learning continued</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Tutorial</td>
                            <td></td>
                            <td>
                                Examples of PCA, k-Means <br/>
                                Bayesian predictive distribution <br/>
                                Bayesian model comparison <br/>
                                <a href="./static/tut7.pdf">pdf slides</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <strong>Week 8</strong>
                            </td>
                            <td></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>Lecture: Mon, Mar 6</td>
                            <td>
                                Mixture of Gaussians: Bishop 9.2  <br/>EM algorithm: Bishop 9.3  
                            </td>
                            <td>
                                <a href="./static/Lec14-15.pdf">Mixture models, EM algorithm</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Assignment 3: Wed, Mar 8</td>
                            <td>
                                Due date: March 24 midnight, 2017 <br/>Unsupervised learning, probablistic models
                            </td>
                            <td>
                                <a href="./static/a3.pdf">Assignment handout (updated Mar 13th)</a>
                                <br/>
                                Download the datasets: <a href="./static/data2D.npy">data2D</a>
                                , <a href="./static/data100D.npy">data100D</a>
                                , <a href="./static/tinymnist.npz">tinymnist</a>
                                <br/>
                                Download the utility function <a href="./static/utils.py">here</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Lecture: Thu, Mar 9</td>
                            <td>
                                Naive Bayes: Hastie et al 2013, Chap. 6.6.3 <br/>Bayesian network: Bishop 8.1, 8.2  
                            </td>
                            <td>
                                <a href="./static/lec15.pdf">Mixture of Gaussians, Naive Bayes and Bayesian Networks</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Tutorial</td>
                            <td></td>
                            <td>
                                Introducing A3 <br/>
                                Examples of Mixture of Bernoullis <br/>
                                EM algorithm <br/>
                                <a href="./static/Tut7.pdf">pdf slides</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <strong>Week 9</strong>
                            </td>
                            <td></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>Lecture: Mon, Mar 13</td>
                            <td>
                                Bishop 8.1 &amp;8.2<br/>
                                Parts of Murphy Ch. 10<br/>
                                Russell and Norvig 2009 (<i>AI: A Modern Approach</i>
                                ) parts of Ch. 14  
                            </td>
                            <td>
                                <a href="./static/Lec16.pdf">Bayesian networks continued</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Lecture: Thu, Mar 16</td>
                            <td>Bishop 8.3, 8.4.3  </td>
                            <td>
                                <a href="./static/Lec17.pdf">Markov Random Fields, factor graphs</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Tutorial</td>
                            <td></td>
                            <td>
                                Review of graphical models <br/>
                                Conversion between BN, MRF and FG <br/>
                                Inference in graphical models <br/>
                                <a href="./static/Tut8.pdf">pdf slides</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <strong>Week 10</strong>
                            </td>
                            <td></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>Lecture: Mon, Mar 20</td>
                            <td>
                                Russell &amp;Norvig 15.1<br/>Parts of Bishop Chap. 13  
                            </td>
                            <td>
                                <a href="./static/Lec18-hmm.pdf">Sequence models</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Lecture: Thu, Mar 23</td>
                            <td>
                                Parts of Russell &amp;Norvig 15.3<br/>Parts of Bishop Chap. 13 
                            </td>
                            <td>
                                <a href="./static/Lec19-hmm2.pdf">Hidden Markov Models (HMMs)</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Tutorial</td>
                            <td></td>
                            <td>
                                Review of Markov models <br/>
                                Examples of inference in graphical models <br/>
                                <a href="./static/Tut9.pdf">pdf slides</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <strong>Week 11</strong>
                            </td>
                            <td></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>Lecture: Mon, Mar 27</td>
                            <td>
                                Murphy 17.4<br/>
                                End of Russell &amp;Norvig 15.2<br/>Bishop 13.2.5  
                            </td>
                            <td>
                                <a href="./static/Lecture20HLI.pdf">HMM inference/learning</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Assignment 4: Mon, Mar 27</td>
                            <td>
                                Due date: April 9th midnight, 2017 <br/>Graphical models, sum-product algorithm
                            </td>
                            <td>
                                <a href="./static/a4.pdf">Assignment handout (updated)</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Lecture: Thu, Mar 30</td>
                            <td>
                                Parts of MacKay Chapter 16 and Sections 26.1-26.2<br/>
                                Bishop 8.4.4 <br/>
                                <a href="www.psi.toronto.edu/~frey/papers/fgspa.ps">Kschischang, Frey and Loeliger: Factor Graphs and the Sum-Product Algorithm</a>
                                Section 2 <br/>
                                <a href="https://arxiv.org/pdf/1212.2486">Frey: Extending Factor Graphs so as to Unify Directed and Undirected Graphical Models</a>
                                Section 2 
                            </td>
                            <td>
                                <a href="./static/Lec21-mp.pdf">Message-passing algorithms (updated notation)</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Tutorial</td>
                            <td></td>
                            <td>
                                Forward-backward algorithm <br/>
                                The sum-product algorithm <br/>
                                <a href="./static/Tut10.pdf">pdf slides</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <strong>Week 12</strong>
                            </td>
                            <td></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>Lecture: Mon, Apr 3</td>
                            <td>
                                Murphy 17.4.4 &amp;20.2<br/>
                                Bishop 8.4.5 &amp;13.2.5<br/>MacKay 26.3
                            </td>
                            <td>
                                <a href="./static/Lec22.pdf">Max-sum algorithm</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Lecture: Thu, Apr 6</td>
                            <td>
                                MacKay Chapters 16 and 26<br/>
                                Bishop 8.4.7<br/>LBP: MacKay 26.4, Bishop 8.4.7
                            </td>
                            <td>
                                <a href="./static/Lec23.pdf">Junction-tree algorithm, Loopy belief propagation</a>
                            </td>
                        </tr>
                        <tr>
                            <td>Tutorial</td>
                            <td></td>
                            <td>
                                Review <br/>
                                <a href="./static/Tut11.pdf">pdf slides</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <strong>Week 13</strong>
                            </td>
                            <td></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>Lecture: Mon, Apr 10</td>
                            <td>
                                <a href="https://www.cs.ubc.ca/~murphyk/MLbook/pml-print3-ch19.pdf">The first section of Murphy 19.6</a>
                            </td>
                            <td>
                                <a href="./static/Lec24.pdf">
                                    Supervised Learning using Graphical Models <br/>
                                    Discriminative Approach <br/>
                                    Conditional Random Fields (CRFs) <br/>Combining Deep Learning with Graphical Models
                                </a>
                            </td>
                        </tr>
                        <tr>
                            <td>Lecture: Thu, Apr 13</td>
                            <td>All the above</td>
                            <td>
                                <a href="./static/Lec25-review.pdf">Course concepts</a>
                                , the 2013 midterm, and finishing our junction-tree algorithm example
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <strong>Exam</strong>
                            </td>
                            <td></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>Thu, Apr 20</td>
                            <td></td>
                            <td>
                                For study practice: <a href="./static/midterm2013.pdf">2013 midterm</a>
                            </td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>
        <hr>
    </main>
    <footer>
        <div class='row'>
            <div class='col-lg-12'>
                <p>ECE521 2017</p>
            </div>
        </div>
    </footer>
</div>
<script src='./assets/app.js'></script>
</body></html>
